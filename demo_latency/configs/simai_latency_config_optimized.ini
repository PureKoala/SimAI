# SimAI GPU Network Latency Simulation Configuration (Optimized)

[simulation]
# Network backend: ns3, analytical, physical
network_backend = ns3
# Simulator path
simulator_binary = auto
# Enable verbose logging
verbose_logging = true

[latency_model]
# Base latency parameters (nanoseconds) - Based on modern GPU hardware specs
gpu_compute_ns = 100
memory_access_ns = 200

# PCIe delay model: Total = Setup + Transfer
# Transfer delay = data_size / bandwidth
pcie_setup_latency_ns = 3000
pcie_effective_bandwidth_gbps = 45

# Alternative interconnect options (uncomment to use)
# nvlink_setup_latency_ns = 800
# nvlink_bandwidth_gbps = 600
# ib_setup_latency_ns = 1500
# ib_bandwidth_gbps = 90

network_base_ns = 1000
switch_processing_ns = 300
protocol_overhead_ns = 100

[test_parameters]
# Default test parameters
comm_types = ["all_reduce", "p2p", "broadcast"]
data_sizes_kb = [1, 4, 16, 64, 256, 1024, 4096]
topologies = ["fat_tree", "dragonfly"]
num_iterations = 5

[network_configs]
# Network configuration
default_bandwidth_gbps = 100      # InfiniBand EDR standard
default_latency_ns = 1000
switch_latency_ns = 100
hca_latency_ns = 200

[output]
# Output configuration
save_plots = true
save_csv = true
save_json = true
results_dir = results

# Parameter Sources and Justification:
# 
# gpu_compute_ns: Based on modern GPU (A100/H100) core frequencies of 1-2GHz
#                 and typical arithmetic operation pipeline depths
#
# memory_access_ns: Based on HBM3 memory specifications, typical latency 150-250ns
#
# pcie_gen4_per_gb_ns: Calculated from PCIe 4.0 x16 theoretical bandwidth of 64GB/s
#                      with realistic 80% efficiency = ~50GB/s effective bandwidth
#                      1GB transfer time = 1GB / 50GB/s = 20ms = 20,000,000ns
#
# network_base_ns: Typical intra-rack communication latency in modern data centers
#
# switch_processing_ns: Modern low-latency data center switches (Mellanox/Broadcom)
#
# protocol_overhead_ns: Optimized RDMA protocols (InfiniBand/RoCE) overhead
