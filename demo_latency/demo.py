#!/usr/bin/env python3
"""
SimAI GPUå»¶è¿Ÿæµ‹è¯•æ¼”ç¤ºè„šæœ¬
Demonstration script for SimAI GPU latency testing

å±•ç¤ºå¦‚ä½•ä½¿ç”¨SimAIè¿›è¡ŒGPUç½‘ç»œå»¶è¿Ÿåˆ†æž
"""

import os
import sys
import time
from pathlib import Path

def print_banner():
    """æ‰“å°æ¨ªå¹…"""
    print("=" * 60)
    print("     SimAI GPUç½‘ç»œå»¶è¿Ÿä»¿çœŸæµ‹è¯•æ¼”ç¤º")
    print("=" * 60)
    print()

def print_step(step_num, description):
    """æ‰“å°æ­¥éª¤"""
    print(f"æ­¥éª¤ {step_num}: {description}")
    print("-" * 40)

def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ç”¨æ³•"""
    print_banner()
    
    print_step(1, "æ£€æŸ¥çŽ¯å¢ƒå’Œä¾èµ–")
    print("æ£€æŸ¥PythonçŽ¯å¢ƒ...")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    try:
        import pandas, numpy, matplotlib, seaborn
        print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {e}")
        print("è¯·è¿è¡Œ: make install-deps")
        return
    
    print("\n" + "="*50)
    time.sleep(2)
    
    print_step(2, "åˆ›å»ºé…ç½®æ–‡ä»¶")
    from simai_latency_simulator import create_default_config
    config_file = create_default_config()
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
    
    print("\n" + "="*50)
    time.sleep(2)
    
    print_step(3, "è¿è¡Œç®€å•å»¶è¿Ÿæµ‹è¯•")
    from simai_latency_simulator import SimAILatencySimulator
    
    # åˆ›å»ºä»¿çœŸå™¨
    simulator = SimAILatencySimulator(config_file)
    print(f"âœ… ä»¿çœŸå™¨å·²åˆå§‹åŒ–")
    print(f"   ç½‘ç»œåŽç«¯: {simulator.network_backend}")
    print(f"   ä»¿çœŸå™¨: {simulator.simulator_binary}")
    
    # ç”Ÿæˆæµ‹è¯•å·¥ä½œè´Ÿè½½
    print("\nç”Ÿæˆæµ‹è¯•å·¥ä½œè´Ÿè½½...")
    workload_file = simulator.generate_workload_file("all_reduce", 65536, 2)
    print(f"âœ… å·¥ä½œè´Ÿè½½æ–‡ä»¶: {workload_file}")
    
    # ç”Ÿæˆç½‘ç»œé…ç½®
    print("\nç”Ÿæˆç½‘ç»œé…ç½®...")
    network_config = simulator.generate_network_config("fat_tree", 100, 1000)
    print(f"âœ… ç½‘ç»œé…ç½®æ–‡ä»¶: {network_config}")
    
    print("\n" + "="*50)
    time.sleep(2)
    
    print_step(4, "è¿è¡Œä»¿çœŸ")
    print("è¿è¡Œå»¶è¿Ÿä»¿çœŸ...")
    result = simulator.run_simulation(workload_file, network_config)
    
    latency_comp = result['latency_components']
    print(f"âœ… ä»¿çœŸå®Œæˆ!")
    print(f"   æ€»å»¶è¿Ÿ: {latency_comp.total_ns:.1f} ns ({latency_comp.total_ns/1000:.2f} Î¼s)")
    print(f"   å»¶è¿Ÿç»„ä»¶:")
    print(f"     GPUè®¡ç®—: {latency_comp.gpu_compute_ns:.1f} ns")
    print(f"     å†…å­˜è®¿é—®: {latency_comp.memory_access_ns:.1f} ns")
    print(f"     PCIeä¼ è¾“: {latency_comp.pcie_transfer_ns:.1f} ns")
    print(f"     ç½‘ç»œä¼ æ’­: {latency_comp.network_propagation_ns:.1f} ns")
    print(f"     äº¤æ¢æœºå¤„ç†: {latency_comp.switch_processing_ns:.1f} ns")
    print(f"     åè®®å¼€é”€: {latency_comp.protocol_overhead_ns:.1f} ns")
    print(f"     åºåˆ—åŒ–: {latency_comp.serialization_ns:.1f} ns")
    
    print("\n" + "="*50)
    time.sleep(2)
    
    print_step(5, "æ¼”ç¤ºæ‰¹é‡æµ‹è¯•")
    print("è¿è¡Œå°è§„æ¨¡æ‰¹é‡æµ‹è¯•...")
    
    # è¿è¡Œå°è§„æ¨¡å»¶è¿Ÿæ‰«æ
    results_df = simulator.run_latency_sweep(
        comm_types=["all_reduce", "p2p"],
        data_sizes=[1024, 4096, 16384],
        topologies=["fat_tree"],
        num_iterations=2
    )
    
    print(f"âœ… æ‰¹é‡æµ‹è¯•å®Œæˆ!")
    print(f"   æ€»æµ‹è¯•æ•°: {len(results_df)}")
    print(f"   å¹³å‡å»¶è¿Ÿ: {results_df['total_latency_us'].mean():.2f} Î¼s")
    print(f"   æœ€å°å»¶è¿Ÿ: {results_df['total_latency_us'].min():.2f} Î¼s")
    print(f"   æœ€å¤§å»¶è¿Ÿ: {results_df['total_latency_us'].max():.2f} Î¼s")
    
    print("\n" + "="*50)
    time.sleep(2)
    
    print_step(6, "åˆ†æžç»“æžœ")
    analysis = simulator.analyze_results(results_df)
    
    print("å»¶è¿Ÿç»„ä»¶åˆ†è§£:")
    for comp, data in analysis['component_breakdown'].items():
        comp_name = comp.replace('_ns', '').replace('_', ' ').title()
        print(f"  {comp_name}: {data['avg_ns']:.1f} ns ({data['percentage']:.1f}%)")
    
    print("\næŒ‰é€šä¿¡ç±»åž‹åˆ†æž:")
    for comm_type, data in analysis['by_comm_type'].items():
        print(f"  {comm_type}: {data['avg_latency_us']:.2f} Î¼s (Â±{data['std_latency_us']:.2f})")
    
    print("\n" + "="*50)
    time.sleep(2)
    
    print_step(7, "ä¿å­˜ç»“æžœ")
    output_file = simulator.save_results(results_df, analysis)
    print(f"âœ… ç»“æžœå·²ä¿å­˜: {output_file}")
    
    # ç”Ÿæˆå›¾è¡¨
    try:
        simulator.generate_plots(results_df)
        print("âœ… å›¾è¡¨å·²ç”Ÿæˆ: results/plots/")
    except Exception as e:
        print(f"âš ï¸  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
    
    print("\n" + "="*50)
    print("ðŸŽ‰ æ¼”ç¤ºå®Œæˆ!")
    print("\nä¸‹ä¸€æ­¥:")
    print("1. æŸ¥çœ‹ç»“æžœæ–‡ä»¶: results/")
    print("2. è¿è¡Œå®Œæ•´æµ‹è¯•: make full-test")
    print("3. è‡ªå®šä¹‰é…ç½®: ç¼–è¾‘ configs/simai_latency_config.ini")
    print("4. æŸ¥çœ‹æ–‡æ¡£: README.md")

def demo_advanced_usage():
    """æ¼”ç¤ºé«˜çº§ç”¨æ³•"""
    print_banner()
    
    print("é«˜çº§ç”¨æ³•æ¼”ç¤º")
    print("=" * 30)
    
    print("\n1. è‡ªå®šä¹‰å»¶è¿Ÿæ¨¡åž‹:")
    print("   ç¼–è¾‘ configs/simai_latency_config.ini ä¸­çš„ [latency_model] éƒ¨åˆ†")
    
    print("\n2. ç½‘ç»œæ‹“æ‰‘å¯¹æ¯”:")
    print("   make topology-test")
    
    print("\n3. æ•°æ®å¤§å°æ‰«æ:")
    print("   make size-sweep-test")
    
    print("\n4. è‡ªå®šä¹‰æµ‹è¯•:")
    print("   make custom-test COMM_TYPES='all_reduce' DATA_SIZES='1024 4096'")
    
    print("\n5. Python APIä½¿ç”¨:")
    print("""
   from simai_latency_simulator import SimAILatencySimulator
   
   simulator = SimAILatencySimulator()
   workload = simulator.generate_workload_file("all_reduce", 65536)
   network = simulator.generate_network_config("fat_tree")
   result = simulator.run_simulation(workload, network)
   """)

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) > 1 and sys.argv[1] == "advanced":
        demo_advanced_usage()
    else:
        demo_basic_usage()

if __name__ == "__main__":
    main()
